{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation : Malimg2022\n",
    "\n",
    "- Step 1 : split data into train and test data in separate folders\n",
    "- Step 2 : Count missing data\n",
    "- Step 3 : Generate missing values per class\n",
    "- Step 4 : Remove extra data\n",
    "- Step 5 : Check new balanced data\n",
    "\n",
    "*Latest update : 04/03/2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : Split data in separate folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*split_folders python library*\n",
    "\n",
    "Installation :  pip install split-folders\n",
    "\n",
    "Documanetation : https://pypi.org/project/split-folders/\n",
    "\n",
    "This library allow us to split database to train, test and evaluation folders separatly for each class in the database\n",
    "Split folders with files (e.g. images) into train, validation and test (dataset) folders.\n",
    "\n",
    "Split with a ratio.\n",
    "To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(\"input_folder\", output=\"output\", seed=1337, ratio=(.8, .1, .1), group_prefix=None) # default values\n",
    "\n",
    "Split val/test with a fixed number of items e.g. 100 for each set.\n",
    "To only split into training and validation set, use a single number to `fixed`, i.e., `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "def split_data(original_data_folder , split_data_folder , test_value):\n",
    "    '''\n",
    "    This function allow us to split the any database into train and validation folders\n",
    "     - original_data_folder : is the path of the original database\n",
    "     - split_data_folder : is the output folder, will be created with the given name\n",
    "     - test_value : a fixed value for validation folder, should be an integer\n",
    "    '''\n",
    "    splitfolders.fixed(original_data_folder, output=split_data_folder, seed=1337, fixed=(test_value), oversample=False, group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_database_path = \"../malimg_paper_dataset_imgs\"\n",
    "output_folder = \"new_output_folder_70\"\n",
    "test_value = 70\n",
    "\n",
    "split_data(original_database_path, output_folder , test_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : Count missing data\n",
    "\n",
    "For each family, we'll check missing data in train folder to know how many images will be generated using various augmentation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def missing_data_counter(train_path, goal_value):\n",
    "    '''\n",
    "    This function calculate number of images in each family.\n",
    "    For a given goal_value, the function calculate number of samples to be generated to reach the goal_value\n",
    "    The goal_value is the number of samples in each class in a balanced database.\n",
    "    \n",
    "    RETURN :: a list of missing value for each class of the given database.\n",
    "    '''\n",
    "    os.chdir(train_path)\n",
    "    list_fams = os.listdir(os.getcwd()) # vector of strings with family names\n",
    "    no_imgs = [] # Get number of samples per family\n",
    "    to_be_augmented = [] # Number of data to be augmented OR missing values\n",
    "    \n",
    "    for i in range(len(list_fams)):\n",
    "        os.chdir(list_fams[i])\n",
    "        len1 = len(glob.glob('*.png'))  # assuming the images are stored as 'png'\n",
    "        no_imgs.append(len1)\n",
    "        # we are going to banlance data with GOAL_VALUE samples per family\n",
    "        missing = goal_value - len1\n",
    "        if missing < 0 :\n",
    "            print(\" \", list_fams[i],\" contains \", len1 , \" ======> xxxxxx remove :  \", missing)\n",
    "        elif missing == 0:\n",
    "            print(\" \", list_fams[i],\" NO ACTION NEEDED\")\n",
    "        else:\n",
    "            print(\" \", list_fams[i],\" contains \", len1 , \" ======> to be augmented :  \", missing)\n",
    "\n",
    "        to_be_augmented.append(missing)\n",
    "        os.chdir('..')\n",
    "        \n",
    "    return to_be_augmented, list_fams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Adialer.C  contains  52  ======> to be augmented :   478\n",
      "  Agent.FYI  contains  46  ======> to be augmented :   484\n",
      "  Allaple.A  contains  2879  ======> xxxxxx remove :   -2349\n",
      "  Allaple.L  contains  1521  ======> xxxxxx remove :   -991\n",
      "  Alueron.gen!J  contains  128  ======> to be augmented :   402\n",
      "  Autorun.K  contains  36  ======> to be augmented :   494\n",
      "  C2LOP.gen!g  contains  130  ======> to be augmented :   400\n",
      "  C2LOP.P  contains  76  ======> to be augmented :   454\n",
      "  Dialplatform.B  contains  107  ======> to be augmented :   423\n",
      "  Dontovo.A  contains  92  ======> to be augmented :   438\n",
      "  Fakerean  contains  311  ======> to be augmented :   219\n",
      "  Instantaccess  contains  361  ======> to be augmented :   169\n",
      "  Lolyda.AA1  contains  143  ======> to be augmented :   387\n",
      "  Lolyda.AA2  contains  114  ======> to be augmented :   416\n",
      "  Lolyda.AA3  contains  53  ======> to be augmented :   477\n",
      "  Lolyda.AT  contains  89  ======> to be augmented :   441\n",
      "  Malex.gen!J  contains  66  ======> to be augmented :   464\n",
      "  Obfuscator.AD  contains  72  ======> to be augmented :   458\n",
      "  Rbot!gen  contains  88  ======> to be augmented :   442\n",
      "  Skintrim.N  contains  10  ======> to be augmented :   520\n",
      "  Swizzor.gen!E  contains  58  ======> to be augmented :   472\n",
      "  Swizzor.gen!I  contains  62  ======> to be augmented :   468\n",
      "  VB.AT  contains  338  ======> to be augmented :   192\n",
      "  Wintrim.BX  contains  27  ======> to be augmented :   503\n",
      "  Yuner.A  contains  730  ======> xxxxxx remove :   -200\n"
     ]
    }
   ],
   "source": [
    "# path to train part of the splitted database\n",
    "train_path_folder = \"C:/Users/Zbook/Desktop/IKRAM_docs/Data_augmentation_code/new_output_folder_70/train/\"\n",
    "my_goal_value = 530 # modify this value if you want more data per family\n",
    "\n",
    "tobeaugmented_values, list_fams = missing_data_counter(train_path_folder, my_goal_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : Generate missing values per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def generate_aug_data(fname, to_be_augmented, list_fams):\n",
    "    '''\n",
    "    Generate augmented data for each family depending on needed values\n",
    "    save the augmented images in the same class folder with a specific prefix for each augmented method\n",
    "    '''\n",
    "\n",
    "    batches = ImageDataGenerator().flow_from_directory(directory=fname, target_size=(64,64), batch_size=8000)\n",
    "    imgs, labels = next(batches)  \n",
    "    \n",
    "    X_train = imgs/255.\n",
    "    y_train = labels\n",
    "    \n",
    "    # Generate augmented data and save them in each correponding folder\n",
    "    for i in range(len(list_fams)):\n",
    "        # absolute path to folder where we save new augmented images\n",
    "        fname_aug = fname + list_fams[i]\n",
    "\n",
    "        if to_be_augmented[i]==0 or to_be_augmented[i]<0 :\n",
    "            continue\n",
    "        \n",
    "        # number of images to be added to_be_augmented[i] OR an integer\n",
    "        #aug_size = int(to_be_augmented[i]*0.25)\n",
    "        \n",
    "        if tobeaugmented_values[i] % 4 == 0:\n",
    "            aug_size = int(to_be_augmented[i]*0.25)\n",
    "        else:\n",
    "            aug_size = int(to_be_augmented[i]*0.25)+1\n",
    "        \n",
    "        # Method 1 : feature standardization \n",
    "        datagen_fs = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "        datagen_fs.fit(X_train)\n",
    "        for X_batch, y_batch in datagen_fs.flow(X_train, y_train, batch_size=aug_size, save_to_dir=fname_aug, save_prefix='aug_fs_', save_format='png'):\n",
    "            break\n",
    "\n",
    "        # Method 2 : Random Rotation \n",
    "        datagen_rot = ImageDataGenerator(rotation_range=90)\n",
    "        datagen_rot.fit(X_train)\n",
    "        for X_batch, y_batch in datagen_rot.flow(X_train, y_train, batch_size=aug_size, save_to_dir=fname_aug, save_prefix='aug_rot_', save_format='png'):\n",
    "            break\n",
    "\n",
    "        # Method 3 : Random shifts \n",
    "        shift = 0.2\n",
    "        datagen_shifts = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift)\n",
    "        datagen_shifts.fit(X_train)\n",
    "        for X_batch, y_batch in datagen_shifts.flow(X_train, y_train, batch_size=aug_size, save_to_dir=fname_aug, save_prefix='aug_shift_', save_format='png'):\n",
    "            break\n",
    "\n",
    "        # Method 4 : Random flips\n",
    "        datagen_flips = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "        datagen_flips.fit(X_train)\n",
    "        for X_batch, y_batch in datagen_flips.flow(X_train, y_train, batch_size=aug_size, save_to_dir=fname_aug, save_prefix='aug_flips_', save_format='png'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7589 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "generate_aug_data(train_path_folder, tobeaugmented_values, list_fams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 : Remove extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def remove_extra_data(fname, missing):\n",
    "    '''\n",
    "    Remove n random images from a specific class/family\n",
    "     - remove_n :: Number of images to be deleted\n",
    "     - path :: class path \n",
    "    '''\n",
    "    \n",
    "    for i in range(len(list_fams)):\n",
    "        if missing[i] < 0:\n",
    "            remove_n = abs(missing[i])\n",
    "            print(remove_n)\n",
    "            family_path = fname + list_fams[i] # family path\n",
    "            files = os.listdir(family_path)  # Get filenames in current folder\n",
    "            files = random.sample(files, remove_n)  # Pick X random files to be deleted\n",
    "            for file in files:  # Go over each file name to be deleted\n",
    "                f = os.path.join(family_path, file)  # Create valid path to file\n",
    "                os.remove(f)  # Remove the file\n",
    "    print(\"DONE !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Adialer.C  contains  532  ======> xxxxxx remove :   -2\n",
      "  Agent.FYI  NO ACTION NEEDED\n",
      "  Allaple.A  contains  2879  ======> xxxxxx remove :   -2349\n",
      "  Allaple.L  contains  1521  ======> xxxxxx remove :   -991\n",
      "  Alueron.gen!J  contains  532  ======> xxxxxx remove :   -2\n",
      "  Autorun.K  contains  532  ======> xxxxxx remove :   -2\n",
      "  C2LOP.gen!g  NO ACTION NEEDED\n",
      "  C2LOP.P  contains  532  ======> xxxxxx remove :   -2\n",
      "  Dialplatform.B  contains  531  ======> xxxxxx remove :   -1\n",
      "  Dontovo.A  contains  532  ======> xxxxxx remove :   -2\n",
      "  Fakerean  contains  531  ======> xxxxxx remove :   -1\n",
      "  Instantaccess  contains  533  ======> xxxxxx remove :   -3\n",
      "  Lolyda.AA1  contains  531  ======> xxxxxx remove :   -1\n",
      "  Lolyda.AA2  NO ACTION NEEDED\n",
      "  Lolyda.AA3  contains  533  ======> xxxxxx remove :   -3\n",
      "  Lolyda.AT  contains  533  ======> xxxxxx remove :   -3\n",
      "  Malex.gen!J  NO ACTION NEEDED\n",
      "  Obfuscator.AD  contains  532  ======> xxxxxx remove :   -2\n",
      "  Rbot!gen  contains  532  ======> xxxxxx remove :   -2\n",
      "  Skintrim.N  NO ACTION NEEDED\n",
      "  Swizzor.gen!E  NO ACTION NEEDED\n",
      "  Swizzor.gen!I  NO ACTION NEEDED\n",
      "  VB.AT  NO ACTION NEEDED\n",
      "  Wintrim.BX  contains  531  ======> xxxxxx remove :   -1\n",
      "  Yuner.A  contains  730  ======> xxxxxx remove :   -200\n",
      "2\n",
      "2349\n",
      "991\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "200\n",
      "DONE !\n"
     ]
    }
   ],
   "source": [
    "# chcek missing update\n",
    "check_missing, list_fams_check = missing_data_counter(train_path_folder, my_goal_value)\n",
    "# then remove extra data\n",
    "remove_extra_data(train_path_folder, check_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 : Check new balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Adialer.C  NO ACTION NEEDED\n",
      "  Agent.FYI  NO ACTION NEEDED\n",
      "  Allaple.A  NO ACTION NEEDED\n",
      "  Allaple.L  NO ACTION NEEDED\n",
      "  Alueron.gen!J  NO ACTION NEEDED\n",
      "  Autorun.K  NO ACTION NEEDED\n",
      "  C2LOP.gen!g  NO ACTION NEEDED\n",
      "  C2LOP.P  NO ACTION NEEDED\n",
      "  Dialplatform.B  NO ACTION NEEDED\n",
      "  Dontovo.A  NO ACTION NEEDED\n",
      "  Fakerean  NO ACTION NEEDED\n",
      "  Instantaccess  NO ACTION NEEDED\n",
      "  Lolyda.AA1  NO ACTION NEEDED\n",
      "  Lolyda.AA2  NO ACTION NEEDED\n",
      "  Lolyda.AA3  NO ACTION NEEDED\n",
      "  Lolyda.AT  NO ACTION NEEDED\n",
      "  Malex.gen!J  NO ACTION NEEDED\n",
      "  Obfuscator.AD  NO ACTION NEEDED\n",
      "  Rbot!gen  NO ACTION NEEDED\n",
      "  Skintrim.N  NO ACTION NEEDED\n",
      "  Swizzor.gen!E  NO ACTION NEEDED\n",
      "  Swizzor.gen!I  NO ACTION NEEDED\n",
      "  VB.AT  NO ACTION NEEDED\n",
      "  Wintrim.BX  NO ACTION NEEDED\n",
      "  Yuner.A  NO ACTION NEEDED\n"
     ]
    }
   ],
   "source": [
    "check_missing, list_fams_check = missing_data_counter(train_path_folder, my_goal_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
